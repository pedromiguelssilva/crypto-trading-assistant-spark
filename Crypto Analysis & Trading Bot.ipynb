{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b9b0667-8e89-4c84-8e4f-e4b69350e74d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cryptocurrency Analysis & Trading Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca8deceb-e57b-4313-86af-3d3bbee185d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note: Clusters 8.3 ML were used in a Databricks Community Edition environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cf50783-80e3-4a81-86a8-e1a9b3ff4272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from requests import Request, Session\n",
    "import json\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "!/databricks/python3/bin/python -m pip install --upgrade pip\n",
    "!pip install yfinance\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8d88dea-ebf1-4450-a234-2899449c7951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the conversion currencies available\n",
    "CONVERSION_CURRENCIES = ['USD', 'EUR', 'GBP', 'JPY', 'AUD']\n",
    "CONVERSION_CURRENCIES_SYM = {'USD': 'US$', 'EUR': '€', 'GBP': '£', 'JPY': '¥', 'AUD': 'A$'} \n",
    "\n",
    "dbutils.widgets.dropdown('Select the conversion currency', 'USD', CONVERSION_CURRENCIES)\n",
    "SEL_CONV_CURR = dbutils.widgets.get('Select the conversion currency')\n",
    "\n",
    "\n",
    "# Select the coins/tokens to work with\n",
    "COINS = ['BTC', 'ETH', 'BNB', 'USDT', 'ADA', 'SOL', 'XRP', 'DOT', 'SHIB', 'DOGE']\n",
    "# We are here working with the top 10 market cap coins (as of November 1st, 2021) for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d012ea8d-04c8-4474-8505-6ac9e029ba7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f54afc88-e6bf-4376-9998-f9756d910a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1. Historical Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "542dd6eb-5bcf-458b-8357-d803889c5aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def yfinance_tickerlist(coins = COINS, conversion_currency = SEL_CONV_CURR):\n",
    "    '''yfinance requires that we assemble a string with all coins for which we want to\n",
    "    call its data. This function grabs the list of coins and produces the string\n",
    "    '''\n",
    "\n",
    "    string = ''\n",
    "\n",
    "    for coin in coins:\n",
    "        # Solana (SOL) and Polkadot (DOT) are represented in Yahoo Finance as SOL1 and DOT1, respectively\n",
    "        if coin in ['SOL', 'DOT']:\n",
    "            string += f'{coin}1-{conversion_currency} '\n",
    "        else:\n",
    "            string += f'{coin}-{conversion_currency} '\n",
    "\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "def new_price_coin_columns(coins = yfinance_tickerlist().split(' ')):\n",
    "    '''yfinance requires that we assemble a string with all coins for which we want to\n",
    "    call its data. This function grabs the list of coins and produces the string\n",
    "    '''\n",
    "\n",
    "    coins = [coin[:-4] for coin in coins]\n",
    "    coins.sort()\n",
    "    nr_coins = len(coins)\n",
    "\n",
    "    price_categories = ['AdjClose', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "    composite_columns = []\n",
    "    for price_category in price_categories:\n",
    "        for coin in coins:\n",
    "            composite_column = f'{price_category}99{coin}' # the separator can not be a special character at the stacking stage below, hence the number (99)\n",
    "            composite_columns.append(composite_column)\n",
    "\n",
    "    return composite_columns\n",
    "\n",
    "\n",
    "def stacking_string(coins = COINS, out_columns_name = \"(Coin, CoinData)\"):\n",
    "    '''Due to the format in which spark dataframes receive the dataframe downloaded from yfinance, we need to unpivot\n",
    "    as many columns as there are coins. This function produces the string used in sql code to do the unpivotting\n",
    "    '''\n",
    "\n",
    "    nr_coins = len(coins)\n",
    "\n",
    "    string_aux = ''\n",
    "    for coin in coins:\n",
    "        string_aux += f\"'{coin}', {coin}, \"\n",
    "\n",
    "    string_aux = string_aux[:-2]\n",
    "\n",
    "    string = f\"stack({str(nr_coins)}, {string_aux}) as {out_columns_name}\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2644a43-d97d-4d5d-b9d9-0be862e86d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Yahoo Finance call returns a Pandas df - let's create a pyspark dataframe from it right away, handle it with Spark and then register it as a temp table \n",
    "df_historicalquotes = spark.createDataFrame(yf.download(tickers = yfinance_tickerlist(), period = '10y', interval = '1d').reset_index()) \\\n",
    "    .toDF('Date', *new_price_coin_columns())\n",
    "\n",
    "# We will use the numerical '99' separator since, apparently, special characters are not allowed\n",
    "df_historicalquotes = df_historicalquotes \\\n",
    "    .select('Date', *[f.col(c).cast(\"float\").alias(c) for c in df_historicalquotes.columns[1:]]) \\\n",
    "    .selectExpr('Date', stacking_string(coins = new_price_coin_columns(),\n",
    "                                     out_columns_name = \"(Category, Value)\")) \\\n",
    "    .withColumn('PriceCategory', f.split(f.col('Category'), '99').getItem(0)) \\\n",
    "    .withColumn('Coin', f.split(f.col('Category'), '99').getItem(1)) \\\n",
    "    .drop('Category') \\\n",
    "    .groupBy('Coin', 'Date') \\\n",
    "    .pivot('PriceCategory') \\\n",
    "    .agg(f.first('Value')) \\\n",
    "    .withColumn('ConversionCurrency', f.lit(SEL_CONV_CURR)) \\\n",
    "    .where((f.col('Volume').isNotNull()) & (f.col('ConversionCurrency') == SEL_CONV_CURR)) \\\n",
    "    .orderBy(f.col('Date').asc(), f.col('Coin').desc())\n",
    "# We chose to agregate by 'first' value but any agregation function would work out here, since there's only value for each combination of Coin, Date and Price Category\n",
    "\n",
    "df_historicalquotes.createOrReplaceTempView(\"HistoricalQuotes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "780462da-91a9-430d-83f2-ad6bed9b6097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.2. Latest Quotes - Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ad2629d-c815-4731-87ac-15d881e03fee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A solution for simulating a streaming scenario is to call CoinMarketData 'LatestQuotes' endpoint every 260 seconds and get a json file with this information for the set of coin we are using. Every json file is stored in a folder in the Databricks FileSystem (dbfs) which is then read by a streaming dataframe to append each new block of data to the previous calls.\n",
    "\n",
    "If we were using the paid version of Databricks, we could schedule a job to call the API every 260 seconds, but since Jobs are not available in Databricks Community Edition, a workaround is to have another notebook \"indefinitely\" running a while loop to call the API every 260 seconds. This is not very elegant though, so we'll leave this piece of code in the end of this very notebook, thus allowing all other cells to run before it and run for as long as one wants it to run, providing new datapoints to our streaming dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "160d54f2-f0d6-448b-a8cd-309169dec672",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2.1. API Requests preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1b18617-1f83-430a-bd2a-4446d51aa23d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# API Request config - latest quotes endpoint\n",
    "URL_ENDPOINT = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n",
    "REQUEST_PARAMETERS = {'symbol': ','.join(COINS),\n",
    "                      'convert': SEL_CONV_CURR}\n",
    "SESSION_HEADERS = {'Accepts': 'application/json',\n",
    "                   'X-CMC_PRO_API_KEY': configfile['api_key']}\n",
    "\n",
    "\n",
    "def cmc_api_request(endpoint = URL_ENDPOINT, parameters = REQUEST_PARAMETERS, headers = SESSION_HEADERS):\n",
    "    '''Function to call CoinMarketCap API and return a json file\n",
    "    with the latest quotes from a list of input crypto coins.\n",
    "    '''\n",
    "\n",
    "    session = Session()\n",
    "    session.headers.update(headers)\n",
    "\n",
    "    response = session.get(endpoint, params = parameters)\n",
    "    resp_dict = json.loads(response.text)\n",
    "\n",
    "    return resp_dict\n",
    "\n",
    "\n",
    "def parsing_response_json():\n",
    "    '''Parsing the json to be read directly by the streaming\n",
    "    dataframe, with field selection.\n",
    "    '''\n",
    "\n",
    "    resp_dict = cmc_api_request()\n",
    "\n",
    "    # Saving the status to append later\n",
    "    status = resp_dict['status']\n",
    "\n",
    "    # Bringing quote keys to the outer structure\n",
    "    for key in resp_dict['data'].keys():\n",
    "        for quote_key in resp_dict['data'][key]['quote'][SEL_CONV_CURR].keys():\n",
    "            resp_dict['data'][key][quote_key] = resp_dict['data'][key]['quote'][SEL_CONV_CURR][quote_key]\n",
    "\n",
    "        #Removing the original quote key + unnecessary fields + dimensional fields, which will be stored in the metadata dataframe\n",
    "        for unnecessary_column in ['quote', 'is_fiat', 'is_active', 'platform', 'date_added', 'name', 'num_market_pairs', 'slug', 'tags', 'max_supply']:\n",
    "            resp_dict['data'][key].pop(unnecessary_column)\n",
    "\n",
    "        #Converting some fields to float to ensure schema consistency\n",
    "        for inconsistent_field in ['circulating_supply', 'total_supply']:\n",
    "            resp_dict['data'][key][inconsistent_field] = float(resp_dict['data'][key][inconsistent_field])\n",
    "\n",
    "    resp_dict = resp_dict['data']\n",
    "    resp_dict['status'] = status\n",
    "\n",
    "    return resp_dict\n",
    "  \n",
    "\n",
    "\n",
    "# Defining the schema from the first call (it's immutable, according to the API Documentation)\n",
    "rdd = spark.sparkContext.parallelize([json.dumps(parsing_response_json())])\n",
    "json_df = spark.read.json(rdd)\n",
    "json_schema = json_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0759f1b9-9cbd-4d69-bb76-427951679045",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2.2. StreamingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe23d9be-d1bd-43e9-ac5a-f17c9f1b8e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_latestquotes = '/FileStore/Dados_Input_ABD/LatestQuotes/'\n",
    "path_streamingcheckpoint = '/tmp/Dados_Input_ABD/StreamingCheckpoint'\n",
    "path_streamingoutput = '/FileStore/Dados_Input_ABD/StreamingOutput'\n",
    "\n",
    "dbutils.fs.mkdirs(path_latestquotes)\n",
    "dbutils.fs.mkdirs(path_streamingcheckpoint)\n",
    "dbutils.fs.mkdirs(path_streamingoutput)\n",
    "\n",
    "\n",
    "streamingDF = spark.readStream \\\n",
    "    .schema(json_schema) \\\n",
    "    .json(path = path_latestquotes) \\\n",
    "    .selectExpr(stacking_string(), \"status\") \\\n",
    "    .withColumn(\"CoinID\", f.col('CoinData').getItem('id')) \\\n",
    "    .withColumn(\"Timestamp\", f.col('status').getItem('timestamp')) \\\n",
    "    .withColumn(\"Price\", f.col('CoinData').getItem(\"price\")) \\\n",
    "    .withColumn(\"ConversionCurrency\", f.lit(SEL_CONV_CURR)) \\\n",
    "    .withColumn(\"TotalSupply\", f.col('CoinData').getItem(\"price\")) \\\n",
    "    .withColumn(\"MarketCap\", f.col('CoinData').getItem(\"market_cap\")) \\\n",
    "    .withColumn(\"FullyDilutedMarketCap\", f.col('CoinData').getItem(\"fully_diluted_market_cap\")) \\\n",
    "    .withColumn(\"MarketCapRank\", f.col('CoinData').getItem(\"cmc_rank\")) \\\n",
    "    .withColumn(\"MarketCapDominance\", f.col('CoinData').getItem(\"market_cap_dominance\")) \\\n",
    "    .withColumn(\"PriceChange1h\", f.col('CoinData').getItem(\"percent_change_1h\")) \\\n",
    "    .withColumn(\"PriceChange24h\", f.col('CoinData').getItem(\"percent_change_24h\")) \\\n",
    "    .withColumn(\"PriceChange7d\", f.col('CoinData').getItem(\"percent_change_7d\")) \\\n",
    "    .withColumn(\"PriceChange30d\", f.col('CoinData').getItem(\"percent_change_30d\")) \\\n",
    "    .withColumn(\"PriceChange60d\", f.col('CoinData').getItem(\"percent_change_60d\")) \\\n",
    "    .withColumn(\"PriceChange90d\", f.col('CoinData').getItem(\"percent_change_90d\")) \\\n",
    "    .withColumn(\"VolumeTraded24h\", f.col('CoinData').getItem(\"volume_24h\")) \\\n",
    "    .withColumn(\"VolumeTradedChange24h\", f.col('CoinData').getItem(\"volume_change_24h\")) \\\n",
    "    .drop('CoinData', 'status') \\\n",
    "    .filter(f.col('ConversionCurrency') == SEL_CONV_CURR)\n",
    "\n",
    "display(streamingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da049241-4e26-4196-9dc4-4193d5924ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streamingDF.writeStream \\\n",
    "    .format('parquet') \\\n",
    "    .option('checkpointLocation', path_streamingcheckpoint) \\\n",
    "    .option('path', path_streamingoutput) \\\n",
    "    .outputMode('append') \\\n",
    "    .trigger(processingTime = '1 second') \\\n",
    "    .start()\n",
    "\n",
    "# Since previous data is not overwritten, we do not need a 'Complete' output mode - 'Append' is faster and enough in this case \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "static_streamingDF = spark.read.parquet(path_streamingoutput)\n",
    "static_streamingDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1a652f-bf4f-471c-b6e4-bc37b5297a02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Quick Insight:** Bitcoin represents more than 41% of the whole crypto market! That's insane!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4274bc4-1389-46ef-b1e1-cdd3fec79e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.3. Coin Metadata\n",
    "\n",
    "Let's now build a dataframe with metadata regarding the selected coins. For that, we will call again CoinMarketCap and Yahoo Finance API's:\n",
    "* Yahoo Finance API provides each coin description;\n",
    "* CoinMarketCap API provides the rest of the metadata we want, including url's for the coin website and social media.\n",
    "After merging both tables on the coin's symbols, we are left with a full detailed dataset on each coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51d6095d-ea45-401a-9202-4b653c2024db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3.1. yfinance for descriptions\n",
    "Since we are only grabbing coin descriptions, any conversion pair (EUR, USD, JPY, etc.) is fine to call the API. Let's use USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69773ef7-b77f-4af9-8d2b-d1d846a9840a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_coinmetadata_yf = '/FileStore/Dados_Input_ABD/CoinMetadata_yf/'\n",
    "dbutils.fs.mkdirs(path_coinmetadata_yf)\n",
    "\n",
    "for coin in yfinance_tickerlist(conversion_currency = 'USD').split(' '):\n",
    "    print(coin[:-4] + ':')\n",
    "\n",
    "    t_coin = yf.Ticker(coin)\n",
    "\n",
    "    dict1 = t_coin.info\n",
    "    yf_metadata_of_interest = ['symbol', 'description']\n",
    "    new_dict = {key: dict1[key] for key in yf_metadata_of_interest}\n",
    "\n",
    "    #Writing the json file to dbfs\n",
    "    dbutils.fs.put(path_coinmetadata_yf + coin[:-4] + '.json', json.dumps(new_dict), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a970aff6-c22d-481d-8710-6d16f3d060fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating an empty dataframe with empty schema\n",
    "df_meta_yf = spark.createDataFrame(data = [], schema = StructType([]))\n",
    "  \n",
    "for json_file in dbutils.fs.ls(path_coinmetadata_yf):\n",
    "  \n",
    "    df_coin_meta = spark.read \\\n",
    "                      .json(json_file.path) \\\n",
    "                      .select('symbol', 'description') \\\n",
    "                      .toDF('Symbol', 'Description')\n",
    "\n",
    "    # If we are handling the first coin, use a copy the dataframe of their dataframe, or else append them\n",
    "    if df_meta_yf.rdd.isEmpty():\n",
    "        df_meta_yf = df_coin_meta.alias('df_meta_yf')\n",
    "    else:\n",
    "        df_meta_yf = df_meta_yf.union(df_coin_meta)\n",
    "\n",
    "# Removing the 1's in SOL and DOT and the '-' + conversion_currency suffix with regular expressions\n",
    "df_meta_yf = df_meta_yf.select(f.regexp_replace(f.regexp_replace('Symbol', r'[0-9]', ''), '-USD', '').alias('Symbol'), 'Description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3406a3b5-2b40-41e7-a0e9-1ebeba163a23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3.2. CoinMarketCap for all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "743b508e-6950-42bc-a257-31a4f0ccbfdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#path_coinmetadata_cmc = '/FileStore/CoinMetadata_cmc/'\n",
    "path_coinmetadata_cmc = '/FileStore/Dados_Input_ABD/CoinMetadata_cmc/'\n",
    "dbutils.fs.mkdirs(path_coinmetadata_cmc)\n",
    "\n",
    "# API Request config - metadata endpoint\n",
    "METAD_URL_ENDPOINT = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/info'\n",
    "METAD_REQUEST_PARAMETERS = {'symbol': ','.join(COINS)}\n",
    "\n",
    "\n",
    "\n",
    "def parsing_cmc_metadata_response_json():\n",
    "  \n",
    "    resp_dict = cmc_api_request(endpoint = METAD_URL_ENDPOINT, parameters = METAD_REQUEST_PARAMETERS)\n",
    "\n",
    "    outer_alt_resp_dict = {}\n",
    "\n",
    "    # Bringing 'urls' keys to the outer structure\n",
    "    for key in resp_dict['data'].keys():\n",
    "        for url_key in resp_dict['data'][key]['urls'].keys():\n",
    "            resp_dict['data'][key][url_key] = resp_dict['data'][key]['urls'][url_key]\n",
    "\n",
    "        #Retaining only the fields of interest\n",
    "        metadata_of_interest = ['id', 'symbol', 'name', 'slug', 'date_added', 'logo', 'platform', 'tag-names', 'website', 'twitter', 'reddit', 'source_code', 'announcement', 'technical_doc']\n",
    "        alt_resp_dict = {met_key: resp_dict['data'][key][met_key] for met_key in metadata_of_interest}\n",
    "\n",
    "        # Handling 'platform' key, which is None for almost every coin in the top 10 market cap list\n",
    "        if alt_resp_dict['platform'] is not None:\n",
    "            alt_resp_dict['platform'] = resp_dict['data'][key]['platform']['name']\n",
    "        else:\n",
    "            alt_resp_dict['platform'] = ''\n",
    "\n",
    "        #Retrieving the values from url fields, which come wrapped in a list\n",
    "        for list_field in ['website', 'twitter', 'reddit', 'source_code', 'announcement', 'technical_doc']:\n",
    "            try:\n",
    "                alt_resp_dict[list_field] = alt_resp_dict[list_field][0]\n",
    "            except:\n",
    "                alt_resp_dict[list_field] = ''\n",
    "\n",
    "        outer_alt_resp_dict[key] = alt_resp_dict\n",
    "\n",
    "    return outer_alt_resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36518090-2802-4731-9487-0d9b9309645f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = 'Metadata_' + datetime.datetime.now().strftime('%d-%m-%Y') + '.json'\n",
    "dbutils.fs.put(path_coinmetadata_cmc + filename, json.dumps(parsing_cmc_metadata_response_json()), True)\n",
    "\n",
    "df_meta_cmc = spark.read \\\n",
    "    .json(path_coinmetadata_cmc + filename) \\\n",
    "    .selectExpr(stacking_string()) \\\n",
    "    .withColumn(\"ID\", f.col('CoinData').getItem('id')) \\\n",
    "    .withColumn(\"Symbol\", f.col('CoinData').getItem('symbol')) \\\n",
    "    .withColumn(\"Name\", f.col('CoinData').getItem('name')) \\\n",
    "    .withColumn(\"Slug\", f.col('CoinData').getItem(\"slug\")) \\\n",
    "    .withColumn(\"DateAdded\", f.col('CoinData').getItem(\"date_added\")) \\\n",
    "    .withColumn(\"Logo\", f.col('CoinData').getItem(\"logo\")) \\\n",
    "    .withColumn(\"Platform\", f.col('CoinData').getItem(\"platform\")) \\\n",
    "    .withColumn(\"Tags\", f.col('CoinData').getItem(\"tag-names\")) \\\n",
    "    .withColumn(\"URL_Website\", f.col('CoinData').getItem(\"website\")) \\\n",
    "    .withColumn(\"URL_Twitter\", f.col('CoinData').getItem(\"twitter\")) \\\n",
    "    .withColumn(\"URL_Reddit\", f.col('CoinData').getItem(\"reddit\")) \\\n",
    "    .withColumn(\"URL_SourceCode\", f.col('CoinData').getItem(\"source_code\")) \\\n",
    "    .withColumn(\"URL_Announcement\", f.col('CoinData').getItem(\"announcement\")) \\\n",
    "    .withColumn(\"URL_TechnicalDoc\", f.col('CoinData').getItem(\"technical_doc\")) \\\n",
    "    .drop('Coin', 'CoinData', 'status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "beac4a28-6f90-4006-ac06-b26b4e321d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3.3. Merging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e1f9245-c6df-42d9-8c8e-07befa4969db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta_cmc.join(df_meta_yf,\n",
    "                           on = 'Symbol',\n",
    "                           how = 'inner')\n",
    "\n",
    "df_meta.createOrReplaceTempView(\"CoinMetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09694de6-071e-4ce4-b131-9e5d55407f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Quick Market Assessment\n",
    "**Before we get too excited about predicting the market** and hopefully make some money for ourselves, let's first plot some important KPI's and \"measure the market temperature\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "730b5dd9-97b6-4f87-8514-4adce068d725",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.1. Computing Returns and Volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e11dfab-1f1f-4d4a-acdd-e9afa99cc4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. We will first eliminate datapoints where Open = 0, which occurs for SHIBA INU, which for long had a price so small that the API simply doesn't show enough digits for it to be higher than 0\n",
    "# Since we are computing returns using the logarithm of Close price divided by Open price, it can't be zero, otherwise we would have indefinite values\n",
    "\n",
    "# 2. Volatilities, being computed as the standard deviation of daily returns, have to be filled with zero for the case we are in the first day of the month (with 1 datapoint only)\n",
    "\n",
    "df_market_assessm = df_historicalquotes \\\n",
    "    .filter(f.col('Open') != 0) \\\n",
    "    .withColumn('DailyReturn', f.log(f.col('Close') / f.col('Open'))) \\\n",
    "    .withColumn(\"Year\", f.year(\"Date\")) \\\n",
    "    .withColumn(\"Month\", f.month(\"Date\")) \\\n",
    "    .groupBy(f.col(\"Year\"), f.col(\"Month\"), f.col(\"Coin\")) \\\n",
    "    .agg(f.round(f.avg(\"Close\"), 4).alias(\"AvgPrice\"), \n",
    "        f.round(f.avg(\"DailyReturn\"), 4).alias(\"AvgReturn\"),\n",
    "        f.round(f.stddev(\"DailyReturn\"), 4).alias(\"Volatility\")) \\\n",
    "    .fillna(0, \"Volatility\") \\\n",
    "    .orderBy(f.col('Coin'), f.col(\"Year\").desc(), f.col(\"Month\").desc()) \\\n",
    "    .select('Coin', 'Year', 'Month', 'AvgPrice', 'AvgReturn', 'Volatility')\n",
    "\n",
    "df_market_assessm.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b34a5537-7b2f-4a6a-8c7c-eebc5b7aed2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To compare the indicators side by side, let's normalize them using SparkML VectorAssembler and Pipeline objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b68da9a-25f3-4cc0-9c95-a82a4c69579d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "cols_to_scale = ['AvgPrice', 'AvgReturn', 'Volatility']\n",
    "\n",
    "# Defining a udf to convert the list output of the transformed object into scalar values\n",
    "list_to_value_udf = f.udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "\n",
    "for column in cols_to_scale:\n",
    "\n",
    "    assembler = VectorAssembler(inputCols = [column], outputCol = column + '_Vect')\n",
    "    scaler = MinMaxScaler(inputCol = column + '_Vect', outputCol = column + '_Scaled')\n",
    "    pipeline = Pipeline(stages = [assembler, scaler])\n",
    "\n",
    "    df_market_assessm = pipeline.fit(df_market_assessm) \\\n",
    "        .transform(df_market_assessm) \\\n",
    "        .withColumn(column + \"_Scaled\", list_to_value_udf(column + \"_Scaled\")) \\\n",
    "        .drop(column + \"_Vect\")\n",
    "\n",
    "\n",
    "df_returns_volatilities = df_market_assessm \\\n",
    "    .groupby('Coin') \\\n",
    "    .agg(f.avg('AvgReturn_Scaled').alias('Return'), f.avg('Volatility_Scaled').alias('Volatility'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d11bfb7-40f0-4077-aefc-b4b8fa0a4b82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apparently, Solana (SOL) has been providing its investors with the highest daily average returns, followed by Polkadot (DOT) and Binance Coin (BNB). This makes sense since all of them are relatively recent coins and therefore experimented higher rails of upward trends before stabilizing just recently.   \n",
    "As for volatilities, three coins are worth mentioning:\n",
    "* USDT has clearly the lowest volatility because it's a stable coin - meaning its indexed to a fiat currency like US dollars or Euros. USDT, in this case, is indexed to US dollars and thus hardly varies at all.\n",
    "* SHIB (Shiba Inu) has the highest volatility. While there may be other factors influencing it, the fact that it is a memecoin (without any technology intent behind) explains most of it, since its price is almost exclusively ruled by investor speculation. While DOGE (Dogecoin) also started as a memecoin, there is currently a developing project behind it, which may explain why it has a lower volatility.\n",
    "* BTC has the second lowest volatility. Again, other factors may explain this fact, but one of them is that Bitcoin is the oldest cryptocurrency and, as we have seen, represents more than 40% of the whole crypto market, and therefore has a higher inertia, and consequently a lower variability/volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6725d0d7-7817-4a02-a3ec-e2d6440f914a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.2. Comparing Crypto with the Stock Market\n",
    "After having handled this crypto data for a while now, one curious detail naturally emerged from it. Indeed, despite all the hype around cryptocurrency lately, the whole crypto market cap does not seem to be that high. Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "341e6c2f-69ea-4c8d-a15e-d563e36fa2ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's use Bitcoin market cap and its dominance to compute the whole crypto market cap\n",
    "# Since it is currently at near all-time highs, it is a good time to do this calculation, but we could also use the absolute maximum\n",
    "whole_crypto_marketcap = static_streamingDF.withColumn('MaxTimestamp', f.max('Timestamp').over(Window.partitionBy('CoinID'))) \\\n",
    "                                           .filter(f.col('Coin') == 'BTC') \\\n",
    "                                           .where(f.col('Timestamp') == f.col('MaxTimestamp')) \\\n",
    "                                           .withColumn('WholeMarketCap', f.col('MarketCap') / (f.col('MarketCapDominance') / 100)) \\\n",
    "                                           .first()['WholeMarketCap']\n",
    "\n",
    "whole_crypto_marketcap = '%.2f' % round(whole_crypto_marketcap / 1000000000000, 2)\n",
    "\n",
    "print(f'Whole Crypto Market Cap: {whole_crypto_marketcap} T {CONVERSION_CURRENCIES_SYM[SEL_CONV_CURR]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f92fb328-04ea-4c15-bc05-c3ae0a31fc73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alright, we are talking in the order of trillions. Let's now compare it with the highest market cap stocks in the S&P 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72b20e7d-ea3a-4c30-8a55-507a8be8aaee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STOCKS_FOR_ASSESSMENT = {'Apple': 'AAPL', 'Microsoft': 'MSFT', 'Google': 'GOOG'}\n",
    "\n",
    "for stock in STOCKS_FOR_ASSESSMENT:\n",
    "  \n",
    "    mc = yf.Ticker(STOCKS_FOR_ASSESSMENT[stock]).info['marketCap']\n",
    "\n",
    "    # if the selected conversion currency is not USD, convert yahoo finance result (mandatorily in USD) to that conversion currency\n",
    "    if SEL_CONV_CURR != 'USD':\n",
    "        conv = yf.Ticker(f'{SEL_CONV_CURR}USD=X').info['previousClose']\n",
    "    mc /= conv\n",
    "\n",
    "    mc = '%.2f' % round(mc / 1000000000000, 2)\n",
    "    print(f'{stock} Market Cap: {mc} T {CONVERSION_CURRENCIES_SYM[SEL_CONV_CURR]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a2a220b-6f94-43c9-a330-39cf48537369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Wow! **Apple market cap is still higher than the whole crypto market cap!**\n",
    "(Had we delivered this work 3 days earlier, Microsoft market cap would be higher too!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9476c09-3adb-4236-b189-e7159780493c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Some Technical Analysis\n",
    "**Now that we are feeling comfortable with this data, let's dig in in some fundamentals and perform some technical analysis.** Technical analysis is used specially by day traders to help define the best moment to entry the market to purchase (or to sell). The number of techniques used by different traders is probably in the order of hundreds or thousands, so let's focus our work in some of the most commonly used.\n",
    "\n",
    "To explain the techniques and assess our calculations, let's grab Cardano (ADA) as our use case (using Bitcoin would be extremely cliché)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ec7c383-bde7-4dda-810a-31f0f2804666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TECH_ANALYSIS_COIN = 'ADA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f4968f7-b425-4def-b5f4-cd0ce9adde92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.1. Crossover with Moving Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91e72a45-2987-4c67-8229-508d1541d623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Moving Averages are one of the most fundamental and useful indicators in technical analysis, since they allow the analyst to identify the general trend of the stock (or crypto) by filtering out short-term noise and (likely) meaningless fluctuations. They can be calculated as **Simple Moving Averages (SMA)**, by simply averaging prices on a defined rolling period, or as Exponential Moving Averages (EMA), by placing a greater weight on more recent datapoints (the closer the higher).\n",
    "\n",
    "Moving Averages have several applications in technical analysis. Crossover, for instance, is a strategy that makes use of two Moving Averages, one for a shorter term and another for a longer term, and advises to buy when the short-term MA is going up and crosses the long-term MA. We typically call this cross a **golden cross**. Conversely, when the short-term MA is going down, it advises to sell as it indicates (according to this strategy) a downward trend - a death cross.\n",
    "\n",
    "Let's use 20 days for the shorter term and 100 for the longer one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe823839-f9cb-40e0-98f4-4aeb70ba7143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's make use of SQL API interval objects to compute and plot moving averages \n",
    "query_moving_averages = \"\"\"\n",
    "    SELECT *,\n",
    "           mean(Close) OVER (\n",
    "                PARTITION BY Coin\n",
    "                ORDER BY CAST(Date AS timestamp) \n",
    "                RANGE BETWEEN INTERVAL 20 DAYS PRECEDING AND CURRENT ROW) AS MovingAvg_20Days,\n",
    "           mean(Close) OVER (\n",
    "                PARTITION BY Coin\n",
    "                ORDER BY CAST(Date AS timestamp) \n",
    "                RANGE BETWEEN INTERVAL 100 DAYS PRECEDING AND CURRENT ROW) AS MovingAvg_100Days\n",
    "    FROM HistoricalQuotes\n",
    "  \"\"\"\n",
    "\n",
    "df_moving_averages = spark.sql(query_moving_averages)\n",
    "df_moving_averages_ex = df_moving_averages.filter(f.col('Coin') == TECH_ANALYSIS_COIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef3793f7-8b17-431a-a079-8430d8f031f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**What do we see?** Focusing on recent times:\n",
    "* We have a golden cross around the beginning of August 2021, which in retrospective was a huge buying tip since Cardano went on to climb to 100% of its price at the time in the following month.\n",
    "* Furthermore, we can see a death cross around the end of October / beginning of November 2021, which again proved to be a good selling tip, since Cardano is still struggling today to come back to the prices of August/ September.\n",
    "\n",
    "Anyway, why look for crosses \"manually\" (in the line plot above) when we can calculate them with spark? Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4923c9d5-dca2-4340-9db1-8a87d07f9f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lag_window = Window.orderBy('Date').partitionBy('Coin')\n",
    "\n",
    "df_crosses = df_moving_averages.withColumn('ShortTermIsHigher', f.when(f.col('MovingAvg_20Days') > f.col('MovingAvg_100Days'), 1).otherwise(0)) \\\n",
    "                               .withColumn('ShortTermIsHigher_Lag', f.lag(f.col('ShortTermIsHigher'), 1).over(lag_window)) \\\n",
    "                               .withColumn('Cross', f.when(f.col('ShortTermIsHigher') > f.col('ShortTermIsHigher_Lag'), 'Golden Cross') \\\n",
    "                                                     .otherwise(f.when(f.col('ShortTermIsHigher') < f.col('ShortTermIsHigher_Lag'), 'Death Cross') \\\n",
    "                                                     .otherwise('No'))) \\\n",
    "                               .filter(f.col('Cross') != 'No') \\\n",
    "                               .select('Coin', 'Date', 'Cross')\n",
    "  \n",
    "df_crosses_ex = df_crosses.filter(f.col('Coin') == TECH_ANALYSIS_COIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "093e334a-f012-4603-91bb-a4ce7c71ecae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2. RSI (Relative Strength Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86d0233b-2c4e-4a4e-9582-d42fd54b454f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "RSI is arguably one of the most used KPI's in technical analysis in the cryptocurrency world (although it is also used for stock assets) to optimize the timing of entering the market for purchasing (or selling). It is fundamentally an indicator of price momentum, ranging from 0 to 100, and typically classifying an asset (in this case, a coin) as overbought (i.e., overvaluated and, consequently, likely set to be corrected downwards) when RSI > 70 and as oversold when RSI < 30.\n",
    "\n",
    "It is formally calculated as:\n",
    "\n",
    "\\\\( RSI = 100 - \\frac{100}{1 + \\frac{Average Gain}{Average Loss}} \\\\)   where the average percentage of gain and losses is usually calculated for a rolling-period of 14 (days, for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2776750b-a478-453a-a6cf-d6f2b402b6c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_rsi_prep = \"\"\"\n",
    "    SELECT Coin,\n",
    "           Date,\n",
    "           AdjClose\n",
    "    FROM HistoricalQuotes\n",
    "  \"\"\"\n",
    "\n",
    "rolling_window = Window.orderBy(f.col('Date').cast('long')).partitionBy('Coin').rowsBetween(-14, 0)\n",
    "\n",
    "# the first 14 rows will be deleted since those points are overinfluenced by the calculations\n",
    "df_rsi = spark.sql(query_rsi_prep) \\\n",
    "              .withColumn('AdjClose_Lag1', f.lag(f.col('AdjClose'), 1).over(lag_window)) \\\n",
    "              .withColumn('AdjClose_Lag1_Diff', f.col('AdjClose') - f.col('AdjClose_Lag1')) \\\n",
    "              .withColumn('MinDate', f.min('Date').over(Window.partitionBy('Coin'))) \\\n",
    "              .where(f.col('Date') > f.col('MinDate')) \\\n",
    "              .withColumn('Up', f.when(f.col('AdjClose_Lag1_Diff') > 0, f.col('AdjClose_Lag1_Diff')).otherwise(0)) \\\n",
    "              .withColumn('Down', f.when(f.col('AdjClose_Lag1_Diff') < 0, f.col('AdjClose_Lag1_Diff') * (-1)).otherwise(0)) \\\n",
    "              .withColumn('RollingUp', f.avg('Up').over(rolling_window)) \\\n",
    "              .withColumn('RollingDown', f.avg('Down').over(rolling_window)) \\\n",
    "              .withColumn('RS', f.col('RollingUp') / f.col('RollingDown')) \\\n",
    "              .withColumn('RSI', 100 - (100 / (1 + f.col('RS')))) \\\n",
    "              .withColumn('Index', f.monotonically_increasing_id()) \\\n",
    "              .filter(f.col('Index') > 14) \\\n",
    "              .select('Coin', 'Date', 'RSI')\n",
    "\n",
    "df_rsi_ex = df_rsi.filter(f.col('Coin') == TECH_ANALYSIS_COIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9510980c-d95a-4d52-bc8f-a74292486636",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. \"Predicting\" the market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39c252f9-1053-48cb-bfff-f771de238c0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.1. Assessing autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcea5be5-5cce-463f-a3eb-0aba0c86ce1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Time series forecasts usually need data to be statistically independent, i.e. non-autocorrelated. Autocorrelation occurs when some (or most) datapoints value depend on the value of other datapoints of the same series. Let's check that through different periods: 1, 10 and 100 (days, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b5f6c81-0029-41a1-af64-18233ca0310b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_autocorr = df_historicalquotes.filter(f.col('Coin') == TECH_ANALYSIS_COIN) \\\n",
    "                                 .withColumn('AdjClose_Lag1', f.lag(f.col('AdjClose'), 1).over(lag_window)) \\\n",
    "                                 .withColumn('AdjClose_Lag10', f.lag(f.col('AdjClose'), 10).over(lag_window)) \\\n",
    "                                 .withColumn('AdjClose_Lag100', f.lag(f.col('AdjClose'), 100).over(lag_window))\n",
    "\n",
    "df_autocorr.select('Coin', 'Date', 'AdjClose', 'AdjClose_Lag1').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9fd167a-594f-4923-aed5-059ae0025623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Short-term data is strongly autocorrelated, it appears!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70af8f8b-5735-4e54-9bcf-14045716ba82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_autocorr.select('Coin', 'Date', 'AdjClose', 'AdjClose_Lag10').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79988d67-0f05-4b59-a595-924de061f1bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**When we look to 10-day differences, though, the autocorrelation starts to vanish, indicating that we can still model our time series reasonably well** (for 100 days, there's virtually zero autocorrelation - go ahead and uncomment the second line to check for yourself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19bb33c8-529d-42b9-b065-5374087f9e03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.2. Assessing stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62379ae5-7820-434e-a7ff-923975e071d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Most time series analysis are built under another assumption, which is that the series mean and variance do not change much over time, i.e., is stationary. Statistical tests such as Dickey-Fuller's are formally used to assess stationarity (or the lack of it), but we can do that directly by having a glance at the chart of any cryptocurrency. Take ADA, for instance: we have seen in chapter 2.1 that there is a clear upward trend along time (and perhaps some light seasonality too), and therefore **the time series is strongly non-stationary**.\n",
    "\n",
    "To solve this problem, let's use subtract consecutive datapoints and model the differences instead of the original values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bdcb4062-c596-4d7b-90e6-0fc8cbabbb9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_forecast = df_historicalquotes.filter(f.col('Coin') == TECH_ANALYSIS_COIN)\n",
    "\n",
    "# Making the data approximately stationary by computing daily returns in the original dataset\n",
    "df_forecast = df_forecast.withColumn('Daily Return', f.round(f.log(f.col('Close') / f.col('Open')), 4))\n",
    "df_forecast.select(\"Date\", \"Daily Return\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b1af51e-d750-4038-83d3-bb03e2744d6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "52ab8b68-ec6f-41a8-8f14-9deae255bb16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "  \n",
    "    df = df.withColumn('Date_dt', f.to_date(f.unix_timestamp(f.col('Date'), 'MM-dd-yyyy').cast(\"timestamp\")))\n",
    "\n",
    "    days = lambda i: i * 86400\n",
    "\n",
    "    w = Window().orderBy(f.col(\"Date\").cast(\"timestamp\").cast(\"long\")) \\\n",
    "              .rangeBetween(-days(10), 0)\n",
    "\n",
    "    lag_window = Window.orderBy(\"Date_dt\")\n",
    "\n",
    "    df = df.withColumn('Daily_Return_0', f.round(f.log(df_forecast.Close / df_forecast.Open), 4)) \\\n",
    "         .withColumn('rolling_average', f.round(f.avg(\"Daily_Return_0\").over(w), 4)) \\\n",
    "         .withColumn('lag_forecast', f.lag(f.col('Daily_Return_0'), -1).over(lag_window))\n",
    "\n",
    "    df = df.select(\"Date\", \"Daily_Return_0\", \"rolling_average\", \"lag_forecast\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_lag(df, n_lags):\n",
    "\n",
    "inputs = [\"Daily_Return_0\"]\n",
    "for i in range(1, n_lags + 1):\n",
    "\n",
    "    w = Window.orderBy(\"Date\")\n",
    "    value_lag = f.lag(f'Daily_Return_{i-1}').over(w)\n",
    "    df = df.withColumn(f'Daily_Return_{i}', value_lag)\n",
    "\n",
    "    inputs.append(f\"Daily_Return_{i}\")\n",
    "    # remove na's created by the lags added\n",
    "    df = df.dropna(how = 'any')\n",
    "\n",
    "return df, inputs\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(df, n_iteration):\n",
    "    # Train Dataset\n",
    "    train_df = df_prepared.where(\"rank <= \" + str(n_iteration)).drop(\"rank\")\n",
    "\n",
    "    # Test Dataset\n",
    "    test_df = df_prepared.where(\"rank > \" + str(n_iteration)).drop(\"rank\")\n",
    "    test_df = test_df.limit(1)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "df_prepared = transform(df_forecast)\n",
    "df_prepared, inputs = add_lag(df_prepared, 6)\n",
    "df_prepared = df_prepared.withColumn(\"rank\", f.percent_rank().over(Window.partitionBy().orderBy(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "547d9a3b-1633-4eef-a467-82f33867fc89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.4. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac37efab-2276-44ea-9c19-0103cca252a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType \n",
    "\n",
    "def random_forest(inputs, df):\n",
    "  \n",
    "    stages = []\n",
    "    inputs.append(\"rolling_average\")\n",
    "    assembler = VectorAssembler(inputCols = inputs, outputCol = \"features\")\n",
    "\n",
    "    # Train a RandomForest model with usual hyperparameters' values\n",
    "    rf = RandomForestRegressor(featuresCol = \"features\", \n",
    "                             labelCol = \"lag_forecast\",\n",
    "                             maxDepth = 5,\n",
    "                             subsamplingRate = 0.8)\n",
    "\n",
    "    # Build stages for pipeline\n",
    "    stages += [assembler, rf]\n",
    "\n",
    "    # Chain assembler and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "\n",
    "    # training set percentage\n",
    "    i = 0.9\n",
    "\n",
    "    schema = StructType([StructField('prediction', DoubleType(), True),\n",
    "                       StructField('lag_forecast', DoubleType(), True),\n",
    "                       StructField('test_rmse', DoubleType(), True)])\n",
    "\n",
    "    forecast = spark.createDataFrame([], schema)\n",
    "\n",
    "    while i < 1:\n",
    "\n",
    "        train_df, test_df = train_test_split(df, i)\n",
    "\n",
    "        pipelineModel = pipeline.fit(train_df)\n",
    "\n",
    "        # Make predictions, access rmse for each point\n",
    "        predictions = pipelineModel.transform(test_df)\n",
    "\n",
    "        predictions = predictions.withColumn('test_rmse', ((predictions.prediction - predictions.lag_forecast) ** 2) ** 0.5) \\\n",
    "                                 .select(\"prediction\", \"lag_forecast\",\"test_rmse\")\n",
    "\n",
    "        forecast = forecast.union(predictions)\n",
    "\n",
    "        i += 0.01\n",
    "\n",
    "    return forecast\n",
    "\n",
    "forecast = random_forest(inputs, df_prepared)\n",
    "forecast.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54b00eaa-e33a-4cb2-bf88-5e4c26b71a2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast.agg(f.avg('test_rmse')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "923f6fec-0764-4e30-92d9-e0238f9d7ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alright, that's not good news. Our average RMSE on the test set is on the order of prices, which is far from optimal. We knew from the beginning, however, that we would hardly be able to build an accurate model for predicting the movements of such a complex system such as the cryptocurrency market (or any other live trading market actually). In fact, predicting the market is such an Holy Graal that, if the model was any good, something would be really wrong.\n",
    "\n",
    "Anyway, again, that's completely not the point of this work. Thus, **we will instead be using the technical analysis indicators to build our trading bot**, right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "025744a5-79d5-47a6-8b7f-fd9c287c0980",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Live Trading Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "491430a5-e6a3-4673-b7a3-1bbe4c0d89a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alright, so far we have implemented some of the most used techniques to help time the market, but we have only seen the results for the historical dataframe. **Now we want to take advantage of this knowledge to build a trading assistance bot that will tell us what to do with each coin (Sell, Buy or Do Nothing), in real-time.**\n",
    "\n",
    "Ideally, we would compute these metrics in the streaming dataframe itself (which would have at least some days as historic), but unfortunately window operations are not (yet) supported in spark streaming dataframes, and therefore we are not able to calculate moving averages, RSI's and forecast time series, all depending on lags. Anyways, since this is for demonstration purposes only, we will use the metrics computed for the historical dataset to provide trading assistance in the streaming df, even though we know that the outcome will only change once per day, even if we have data for every 5 minutes.\n",
    "\n",
    "We will be using a simple model for deciding what to do with each coin:\n",
    "* **BUY**:\n",
    "  * Main Condition: If there's been a Golden Cross in the past x days;\n",
    "  * Secondary Condition: If the RSI has been below 30 at least once in the past y days.\n",
    "\n",
    "* **SELL**:\n",
    "  * Main Condition: If there's been a Death Cross in the past x days;\n",
    "  * Secondary Condition: If the RSI has been above 70 at least once in the past y days.\n",
    "  \n",
    "* **DO NOTHING**:\n",
    "  * If neither BUY nor SELL conditions apply.\n",
    "  \n",
    "*Note 1: Main and Secondary conditions mean that the Main Condition will be evaluated first for BUY/SELL, and then the Secondary. This is to avoid contraditory cases where for instance a Coin had a Golden Cross in the past few days, indicating BUY, but also had the RSI above 70, indicating SELL.*   \n",
    "*Note 2: x and y are number of days defined by the user.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b812fe6-2fa8-4aed-bf6f-3e5f0f224ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CROSS_CUTOFF_DAYS = 7\n",
    "\n",
    "# Dataframe with the last Golden and Death Crosses by Coin and days passed since\n",
    "df_lastcross = df_crosses \\\n",
    "    .withColumn('Coin', f.regexp_replace('Coin', '1', '')) \\\n",
    "    .groupBy('Coin', 'Cross') \\\n",
    "    .agg(f.max('Date').alias('LastTime')) \\\n",
    "    .groupBy('Coin') \\\n",
    "    .pivot('Cross') \\\n",
    "    .agg(f.first('LastTime')) \\\n",
    "    .toDF('Coin', 'LastDeathCross', 'LastGoldenCross') \\\n",
    "    .withColumn(\"CurrentDate\", f.current_date()) \\\n",
    "    .withColumn(\"DaysSinceLastDeathCross\", f.datediff(f.col(\"CurrentDate\"), f.col(\"LastDeathCross\"))) \\\n",
    "    .withColumn(\"DaysSinceLastGoldenCross\", f.datediff(f.col(\"CurrentDate\"), f.col(\"LastGoldenCross\"))) \\\n",
    "    .select('Coin', 'DaysSinceLastDeathCross', 'DaysSinceLastGoldenCross')\n",
    "\n",
    "\n",
    "RSI_CUTOFF_DAYS = 7\n",
    "\n",
    "# Dataframe with maximum and minimum RSI's in the last n days (defined in the variable RSI_CUTOFF_DAYS above), by Coin\n",
    "df_minmaxrsi = df_rsi \\\n",
    "    .withColumn('Coin', f.regexp_replace('Coin', '1', '')) \\\n",
    "    .withColumn(\"current_date\", f.current_date()) \\\n",
    "    .withColumn(\"diff_in_days\", f.datediff(f.col(\"current_date\"), f.col(\"Date\"))) \\\n",
    "    .filter(f.col(\"diff_in_days\") <= RSI_CUTOFF_DAYS) \\\n",
    "    .groupBy('Coin') \\\n",
    "    .agg(f.max('RSI').alias(f'MaxRSI_{RSI_CUTOFF_DAYS}days'), f.min('RSI').alias(f'MinRSI_{RSI_CUTOFF_DAYS}days'), f.avg('RSI').alias(f'AvgRSI_{RSI_CUTOFF_DAYS}days'))\n",
    "\n",
    "\n",
    "# Final Dataframe with the Recomendations\n",
    "df_assistbot = static_streamingDF \\\n",
    "    .alias('df_assistbot') \\\n",
    "    .withColumn('Timestamp', f.expr(\"substring(Timestamp, 1, length(Timestamp) - 5)\")) \\\n",
    "    .withColumn('Timestamp', f.regexp_replace('Timestamp', 'T', ' ')) \\\n",
    "    .withColumn('Timestamp', f.to_timestamp(f.col('Timestamp'), 'yyyy-MM-dd HH:mm:ss').alias('Timestamp')) \\\n",
    "    .withColumn('MaxTimestamp', f.max('Timestamp').over(Window.partitionBy('Coin'))) \\\n",
    "    .where(f.col('Timestamp') == f.col('MaxTimestamp')) \\\n",
    "    .select('Coin', 'Timestamp', 'Price') \\\n",
    "    .join(df_lastcross, on = 'Coin', how = 'inner') \\\n",
    "    .join(df_minmaxrsi, on = 'Coin', how = 'inner')\n",
    "\n",
    "df_assistbot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0390b93c-8112-471b-ba79-87568bcb90a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_condition_to_buy = f.col('DaysSinceLastGoldenCross') <= CROSS_CUTOFF_DAYS\n",
    "sec_condition_to_buy = f.col(f'MinRSI_{RSI_CUTOFF_DAYS}days') <= 30\n",
    "\n",
    "main_condition_to_sell = f.col('DaysSinceLastDeathCross') <= CROSS_CUTOFF_DAYS\n",
    "sec_condition_to_sell = f.col(f'MaxRSI_{RSI_CUTOFF_DAYS}days') >= 70\n",
    "\n",
    "\n",
    "df_assistbot = df_assistbot.withColumn('Recommendation', f.when(main_condition_to_buy, 'BUY') \\\n",
    "                                                          .otherwise(f.when(main_condition_to_sell, 'SELL') \\\n",
    "                                                          .otherwise(f.when(sec_condition_to_buy, 'BUY') \\\n",
    "                                                          .otherwise(f.when(sec_condition_to_sell, 'SELL') \\\n",
    "                                                          .otherwise('-'))))) \\\n",
    "                           .select('Coin', 'Timestamp', 'Recommendation')\n",
    "\n",
    "\n",
    "# Let us use the show method this time to see all coins at once\n",
    "df_assistbot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4dcfa9a-290e-4396-ab59-b0d8ae90ef62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Appendix - Calling CoinMarketCAP API LatestQuotes endpoint, recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1359bca-c515-43c2-b493-2df838f5ede1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    filename = 'Top10MarketCap_' + datetime.datetime.now().strftime('%d-%m-%Y-%Hh%M') + '.json'\n",
    "    dbutils.fs.put(path_latestquotes + filename, json.dumps(parsing_response_json()), True)\n",
    "\n",
    "    time.sleep(260)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Crypto Analysis & Trading Bot",
   "notebookOrigID": 2761854724519240,
   "widgets": {
    "Select the conversion currency": {
     "currentValue": "USD",
     "nuid": "b7f0b816-0b8a-4244-ab5f-356d47b7c654",
     "widgetInfo": {
      "defaultValue": "USD",
      "label": null,
      "name": "Select the conversion currency",
      "options": {
       "choices": [
        "USD",
        "EUR",
        "GBP",
        "JPY",
        "AUD"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
